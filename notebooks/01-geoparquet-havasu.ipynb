{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59edb5cf-140d-4088-aef4-9612f902cffd",
   "metadata": {},
   "source": [
    "# GeoParquet & Havasu Iceberg With SedonaDB & Wherobots Cloud\n",
    "\n",
    "This notebook explores the GeoParquet file format for working with vector data and also the Havasu Iceberg table format.\n",
    "\n",
    "* Examining contents of a GeoParquet file\n",
    "* Loading GeoParquet files in SedonaDB\n",
    "* Creating and querying GeoParquet using SedonaDB\n",
    "* Options for spatial partitioning of GeoParquet files with performance comparisons\n",
    "* Working with Havasu Iceberg tables \n",
    "\n",
    "\n",
    "This notebook is intended to be run in [Wherobots Cloud](https://wherobots.serices). Anyone can [create a free account](https://wherobots.services) to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dec50-39d8-45c1-9095-205c14f6fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other dependencies are intsalled by default in Wherobots Cloud\n",
    "\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38e235-e3f8-40fc-8892-31cdae4be11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet\n",
    "import json\n",
    "import os\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import expr\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.utils.adapter import Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65df01f-05e1-4415-b579-ab45015650cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GeoParquet\n",
    "\n",
    "[GeoParquet](https://geoparquet.org) is a specification the defines how to storage vector geospatial data using Apache Parquet. Parquet is a columnar data format commonly used with cloud object stores like AWS S3 for efficent storage and retrieval of large scale data. The main concepts of GeoParquet include:\n",
    "\n",
    "* Specifying how geometries (points, lines, polygons) should be serialized in Parquet files (WKB) and specifying a new Parquet type `geometry`\n",
    "* Additional metadata that describes the geospatial data stored in each Parquet file\n",
    "\n",
    "### GeoParquet Metadata & Geometries\n",
    "\n",
    "The [GeoParquet specification ](https://github.com/opengeospatial/geoparquet) specifies the following types of metadata:\n",
    "\n",
    "* File metadata\n",
    "    * **version**, **primary_column**, **columns**\n",
    "* Column metadata\n",
    "    * **encoding (WKB)**, **geometry_types**, crs, orientation, edges, bbox, epoch\n",
    "    \n",
    "    \n",
    "Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee409e-4e54-41a7-afd7-5b84d8640512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example GeoParquet file \n",
    "\n",
    "!wget https://github.com/opengeospatial/geoparquet/raw/main/examples/example.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a2ea6c-66e8-4214-bcfa-62a3322e1cb6",
   "metadata": {},
   "source": [
    "We can inspect the metadata included in this GeoParquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b7438-6050-4ed3-8e56-ed9a79bd63d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = pa.parquet.read_table('./example.parquet')\n",
    "metadata_str = table.schema.metadata[b'geo'].decode('utf-8')\n",
    "metadata = json.loads(metadata_str)\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465f236-0b9b-4968-882c-9e245d170b95",
   "metadata": {},
   "source": [
    "And to view the schema and GeoParquet file contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3da40a-6511-4524-bf4c-4e5021688455",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0ecfe-5d60-46f0-83ed-02bd95b36618",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load GeoParquet File In SedonaDB\n",
    "\n",
    "GeoParquet is compatible with any Parquet reader, however readers that implement GeoParquet are able to take advantage of the `geometry` type serialization and associated metadata. Let's load the example GeoParquet file in [SedonaDB.](https://wherobots.com/sedona-db/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf792f60-b950-4d58-9146-dc1c6dbc2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SedonaContext, specify credentials for AWS S3 bucket(s) (optional)\n",
    "\n",
    "config = SedonaContext.builder(). \\\n",
    "    config(\"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d9b2a-eea4-4d1c-acb5-1d1c49e7b2ab",
   "metadata": {},
   "source": [
    "We'll load the GeoParquet file directly into a SedonaDB Spatial DataFrame and specify a named view for the DataFrame to enable querying using Spatial SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab1a12-5d9c-46e9-8a19-a0e7efc08075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sedona.read.format(\"geoparquet\").load(\"s3://wherobots-examples-prod/data/geoparquet/example.parquet\")\n",
    "df.createOrReplaceTempView(\"example\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41814c-56a1-4b42-8cc2-c8113d075ba8",
   "metadata": {},
   "source": [
    "We can also inspect the schema of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396154a-3555-4f28-8dec-c883184852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ce67f-d534-4385-bdec-05a3ecbeade9",
   "metadata": {},
   "source": [
    "We can visualize this data using SedonaKepler, the Kepler.gl integration for SedonaDB and Apache Sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458da2f-3f02-41a2-a285-df3f77cce09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaKepler.create_map(df, name=\"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6eb15d-b9fb-4b71-81af-1c816647077d",
   "metadata": {},
   "source": [
    "One of the benefits of GeoParquet is that geometries are loaded directly as geometry types, we don't need to explicitly cast or create them and can directly start working with geometries. Here we use the `ST_Centroid` Spatial SQL function to calculate centroids for each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca58077-e2b3-4d2e-8053-95b0a245961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"\"\"\n",
    "SELECT *, ST_Centroid(geometry) AS centroid\n",
    "FROM example\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95bcd3-d710-4538-a1cd-ea5e273316d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating GeoParquet Files Using SedonaDB\n",
    "\n",
    "Now that we've seen how to load GeoParquet files in SedonaDB, let's see how to create GeoParquet files. We'll use sample data from [BirdBuddy smart birdfeeders](https://live.mybirdbuddy.com/) that record bird species observations. We'll load this data from a public S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e6f31-b3e4-4111-bd46-126e7e45228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_S3_URL = \"s3://wherobots-examples-prod/data/examples/birdbuddy_oct23.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e9dbe-c31b-4882-8161-674f3c236b37",
   "metadata": {},
   "source": [
    "The BirdBuddy data is distributed as CSV so we'll need to explicitly convert numeric latitude and longitude fields into a point geometry using the `ST_Point` spatial SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c1422-57a2-44d4-9a1b-3f01aacd9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df = sedona.read.format('csv').option('header','true').option('delimiter', ',').load(BB_S3_URL)\n",
    "bb_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a33628-c454-4299-84dc-73fca01b27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df = bb_df.selectExpr('ST_Point(CAST(anonymized_longitude AS float), CAST(anonymized_latitude AS float)) AS location', 'CAST(timestamp AS timestamp) AS timestamp', 'common_name', 'scientific_name')\n",
    "bb_df.createOrReplaceTempView('bb')\n",
    "bb_df.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8868-51a3-4c7a-8e80-c5663662153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cefd9c-ace4-45b9-914f-45609f2eedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1916a1-419f-4c40-9e8e-d2cdf5c2082f",
   "metadata": {},
   "source": [
    "We can explore and visualize the data, for example let's find all Junco observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982e2dc-afb1-4de7-bfa6-74c3688e9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "chickadee_df = sedona.sql(\"SELECT * FROM bb WHERE common_name = 'mountain chickadee' \")\n",
    "chickadee_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa975f-f7ef-4bd5-a159-e49b1cf0752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaKepler.create_map(df=chickadee_df, name='Mountain Chickadee')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651586ba-4224-42a7-bf42-80167624e7c7",
   "metadata": {},
   "source": [
    "A common analysis that we might want to explore with this data is calculating the range of each species. We can do this using the `ST_ConvexHull` spatial SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e34a4-fc5a-403f-a951-f8c6e8b3cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df = sedona.sql(\"\"\"\n",
    "    SELECT common_name, COUNT(*) AS num, ST_ConvexHull(ST_Union_aggr(location)) AS geometry \n",
    "    FROM bb \n",
    "    WHERE common_name IN ('california towhee', 'steller’s jay', 'mountain chickadee', 'eastern bluebird') \n",
    "    GROUP BY common_name \n",
    "    ORDER BY num DESC\n",
    "\"\"\")\n",
    "range_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592c095-8c8f-4a84-aa8f-47e0098a860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaKepler.create_map(df=range_df, name=\"Bird species range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ed0ee-dd86-40e0-8120-a00e1d2de0d6",
   "metadata": {},
   "source": [
    "Now, let's save the BirdBuddy observation data as GeoParquet, then compare the performance of loading and querying the GeoParquet version of this dataset to the CSV format. We can use SedonaDB's built in GeoParquet writer. Note that we repartition to save a *single* GeoParquet file. We'll explore partitioned GeoParquet next. \n",
    "\n",
    "But first we need to configure where we'll be saving our GeoParquet files. By default every Wherobots Cloud account includes access to a private AWS S3 cloud storage bucket accessible to each specific Wherobots Cloud user. We can find the S3 URL for our private S3 storage using the `USER_S3_PATH` environment variable. You can also access, manage, and upload data via the Wherobots Cloud File Browser.\n",
    "\n",
    "![](https://wherobots.com/wp-content/uploads/2024/01/Wherobots_File_Browser.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024f76d-364c-414a-8976-1feccbdd7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_S3_PATH = os.environ.get(\"USER_S3_PATH\")\n",
    "print(USER_S3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69055068-58eb-495e-801b-08fe809a851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df.repartition(1).write.mode(\"overwrite\").format(\"geoparquet\").save(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf45e2-e767-4469-a173-07831ecd6064",
   "metadata": {},
   "source": [
    "One of the benefits of GeoParquet is efficient data storage thanks to Parquet's encoding and compression. If we compare file sizes to the original CSV:\n",
    "\n",
    "* CSV format: 425 MB\n",
    "* GeoParquet format: 40 MB \n",
    "\n",
    "Let's load this GeoParquet file in SedonaDB and compare the performance of a spatial filter query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04649dbd-aaf3-43e5-971f-d5c91c0923e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_pq = sedona.read.format('geoparquet').load(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d2bac-a4ab-49c7-8416-26a6fb4d013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_pq.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa3fcc-8c8e-4ea4-9109-5ae30d72b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a spatial filter to find bird observations within a bounding box\n",
    "\n",
    "spatial_filter = \"ST_Within(location, ST_PolygonFromEnvelope(-112.473156, 33.179649, -111.502991, 33.868652))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8871888-c065-44bd-b6ae-86b3bea54568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02d446-307a-4f3c-bd2f-6867bf2488c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bb_df.where(spatial_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2fd749-cbcf-4cd0-bec6-8cebe1971836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76581dd-5539-4dcf-952d-ed50b7f9af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bb_df_pq.where(spatial_filter).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f740d-afbf-42e4-8fbe-e1fb97d47215",
   "metadata": {},
   "source": [
    "### Partitioned GeoParquet\n",
    "\n",
    "Can we further improve performance by creating Partitioned GeoParquet files? \n",
    "\n",
    "First, we'll need to decide on a partitioning strategy. Options for partioning include:\n",
    "\n",
    " * Geohash\n",
    " * S2 index\n",
    " * Administrative boundary\n",
    " \n",
    " \n",
    " Choosing which partitioning option is an exercise is balancing the number of data files and the spatial distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6b1b1-c3d8-4ba8-94b6-b241ec27fca2",
   "metadata": {},
   "source": [
    "#### Geohash partitioned GeoParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a29756-c4eb-48c6-a3a2-1037803a8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_geohash = bb_df.withColumn(\"geohash\", expr(\"ST_GeoHash(location, 2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95031882-fca9-4c0f-aded-410a0e5f64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_geohash.repartition(\"geohash\").write.mode(\"overwrite\").partitionBy(\"geohash\").format(\"geoparquet\").save(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy_geohash.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7e4b4-7120-4135-afe4-07e4290c2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_geohash_pq = sedona.read.format(\"geoparquet\").load(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy_geohash.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bbc72-ec59-4fe9-8863-06bc95afb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bb_df_geohash_pq.where(spatial_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c2ea4-6bd7-431e-9926-804fb272a5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d27a8fd6-5fb1-4065-8060-1b7c32963e13",
   "metadata": {},
   "source": [
    "#### S2 Index Partitioned GeoParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d56c0a-5ded-4e7c-b24b-7965df3c7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_pq = bb_df.withColumn(\"s2\", expr(\"array_max(ST_S2CellIds(location, 2))\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57daf1a-7a5a-4ee6-a249-8034575314f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_pq.repartition(\"s2\").write.mode(\"overwrite\").partitionBy(\"s2\").format(\"geoparquet\").save(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy_s2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3df034-1497-4db1-b5c2-d10828e6472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df_s2 = sedona.read.format(\"geoparquet\").load(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy_s2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b3fa1-7d03-447e-8763-5b6912394d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bb_df_s2.where(spatial_filter).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c85f2-591a-4560-94bc-7f9fac6c3c1d",
   "metadata": {},
   "source": [
    "#### Administrative Boundary Partitioned GeoParquet\n",
    "\n",
    "Requires a spatial join with an administrative boundary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bc1ba-6233-4365-87f4-b108540d2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_NE_ADMIN1_URL = \"s3://wherobots-examples-prod/data/ne_10m_admin_1_states_provinces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e17c8-9f4c-4b26-ad2d-e73dcbd23771",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialRDD = ShapefileReader.readToGeometryRDD(sedona, S3_NE_ADMIN1_URL)\n",
    "admin_df = Adapter.toDf(spatialRDD, sedona)\n",
    "admin_df.createOrReplaceTempView(\"admins\")\n",
    "admin_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a10ac-e453-415b-8d27-5d4e6f69d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d14aba-2a2b-4d53-95de-f79a03afbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_admin_df = sedona.sql(\"\"\"\n",
    "SELECT bb.location AS location, bb.timestamp AS timestamp, bb.common_name AS common_name, bb.scientific_name AS scientific_name, admins.iso_3166_2 AS state \n",
    "FROM bb\n",
    "JOIN admins \n",
    "WHERE ST_Intersects(admins.geometry, bb.location)\n",
    "\"\"\").repartition(\"state\")\n",
    "\n",
    "bb_admin_df.createOrReplaceTempView(\"bb_admin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0f1c4-aabe-4275-9c38-962276c4921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_admin_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405774d0-aa5c-4bd9-9d6b-e92af4831019",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"\"\"\n",
    "WITH distinct_states AS (SELECT DISTINCT state FROM bb_admin)\n",
    "SELECT COUNT(*) AS num FROM distinct_states\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba42c3-65ba-4d6e-a584-18bba20a30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_count_df = sedona.sql(\"\"\"\n",
    "WITH states_count AS (SELECT COUNT(*) AS num, state\n",
    "FROM bb_admin\n",
    "GROUP BY state\n",
    "ORDER BY num DESC)\n",
    "SELECT states_count.num, states_count.state, admins.geometry FROM states_count\n",
    "JOIN admins ON states_count.state = admins.iso_3166_2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f2d73-ff44-4fe1-b35a-e6665be02166",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaKepler.create_map(state_count_df, name=\"Bird observations by state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851e83c-8243-46ed-88a0-cd9b8d52da5a",
   "metadata": {},
   "source": [
    "Partitioning by state/province is probably not the best option given the number of data files vs the size of the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ea376-9710-4455-abcb-57e52859e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb_admin_df.write.mode(\"overwrite\").partitionBy(\"state\").format(\"geoparquet\").save(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy_state.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a143ce-cffe-412c-8378-0427dd5ceacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb_admin_state_df = sedona.read.format(\"geoparquet\").load(USER_S3_PATH + \"geoparquet/\" + \"birdbuddy_state.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd113a30-219d-428b-9534-6a545eabb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#bb_admin_state_df.where(spatial_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540a8a5-9a4c-49f7-9225-3f19a47a412d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d43df-2a84-4d82-85ef-fbad311185ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd7f87fd-29bf-4783-aec2-7afd261c2f50",
   "metadata": {},
   "source": [
    "## Creating Spatial Data Lakehouse With Apache Iceberg Havasu\n",
    "\n",
    "We've seen the benefit of working with GeoParquet, but how do we handle new data updates? What if wanted the developer experience of a database for our GeoParquet files? This is exactly what table formats enable - the \"Data Lakehouse\", being able to work with a Data Lake (in this case GeoParquet files) like the data warehouse database. We'll make use of the Havasu table format, which extends Apache Iceberg to offer:\n",
    "\n",
    "* ACID transactions\n",
    "* schema evolution\n",
    "* time travel\n",
    "\n",
    "on spatial data, including geometries and raster data.\n",
    "\n",
    "Let's see Havasu in action by creating a table using our BirdBuddy observation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa3a79-5e2d-46f6-8a98-583af50eaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"DROP TABLE IF EXISTS wherobots.birdbuddy.observations\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccbdc65-113c-439f-938e-ff692dd16f0f",
   "metadata": {},
   "source": [
    "The catalog is the top-level namespace in Havasu. By default Whereobots Cloud is configured to use the wherobots catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86cfff-2849-410f-a2f3-02975fbaf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146f33b-f840-4461-829f-49eb6b2a114a",
   "metadata": {},
   "source": [
    "Let's create a table that matches the schema of our BirdBuddy observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ff87b-1264-4b44-858e-9bd1840f5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS wherobots.birdbuddy.observations (\n",
    "location GEOMETRY,\n",
    "timestamp STRING,\n",
    "common_name STRING,\n",
    "scientific_name STRING\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979abd4f-1dc2-4e17-958e-02ee149f25e7",
   "metadata": {},
   "source": [
    "We can view all databases (or schemas) in the `wherobots` catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c2e6b-1d9f-4a57-b698-0ceed306de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"SHOW SCHEMAS IN wherobots\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52286e7-ee17-4f96-afa4-c0d3146d63d6",
   "metadata": {},
   "source": [
    "Similarly, we can view all tables in a given schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385346c-679e-4f6e-8762-da06413a85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"SHOW TABLES IN wherobots.birdbuddy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffab26a-7999-4a20-9e8f-ea382a6e6232",
   "metadata": {},
   "source": [
    "And we can view the schema of each table. Note the `geometry` data type - this is introduced by Havasu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed4d53-bf4c-4053-88b9-bb84c4d33543",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"DESCRIBE TABLE wherobots.birdbuddy.observations\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9d106-fabd-49a9-b168-6c6b067cad7f",
   "metadata": {},
   "source": [
    "We can insert data into our table using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e286895-5a64-49f0-83f3-9384d1f49a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"\"\"\n",
    "INSERT INTO wherobots.birdbuddy.observations\n",
    "VALUES (ST_GeomFromText('POINT (-73.96969 40.749244)'), current_timestamp(), 'purple finch', 'haemorhous purpureus')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0252d5-77b0-4abe-9480-faedf9a85fcf",
   "metadata": {},
   "source": [
    "And we can use SQL to query our Havasu tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6ea92-8426-42c0-bb21-2284e15275c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = sedona.sql(\"\"\"\n",
    "SELECT * FROM wherobots.birdbuddy.observations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ba9e8-0513-49e6-946f-cc237ab9a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaKepler.create_map(obs_df, name=\"Observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0815f-48f9-4daa-9172-a896617af7ac",
   "metadata": {},
   "source": [
    "We can also write Sedona Spatial DataFrames to Havasu tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01e1a9-b944-4382-8284-cbea165b56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_df.writeTo(\"wherobots.birdbuddy.observations\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afedbd-1bbb-4d47-bd81-78242240c65e",
   "metadata": {},
   "source": [
    "Havasu supports spatial filter push-down by recording spatial statistics of data files, which includes the bounding box of geometries in each data file. At query time data files can be excluded based on this bounding box. For best performance we will cluster our table by spatial proximity based on the geometry column. Havasu enables this by supporting a Hilbert curve based index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e679cc-4e94-4a5e-8fea-b6da2cd5450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"CREATE SPATIAL INDEX FOR wherobots.birdbuddy.observations USING hilbert(location, 10)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8d669-275d-45c9-9ce6-ea20132d15de",
   "metadata": {},
   "source": [
    "We can query our table using Spatial SQL functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6eb40-a44d-4811-94da-fb5656d1525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_df = sedona.sql(\"\"\"\n",
    "SELECT * FROM wherobots.birdbuddy.observations\n",
    "WHERE ST_Intersects(location, ST_Buffer(ST_GeomFromText('POINT (-73.96969 40.749244)'), 0.25))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35fef04-a4cf-4afe-bc24-43b60e5f9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "SedonaKepler.create_map(buffer_df, \"Bird observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396f7ce-5092-40ad-ab34-f6ab4738e847",
   "metadata": {},
   "source": [
    "Let's compare the performance of our Havasu table to the other formats using the same spatial filter query as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b96f40-6b2f-48bd-aa00-9d6da9fa480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filter_df = sedona.sql(\"\"\"\n",
    "SELECT COUNT(*) AS num FROM wherobots.birdbuddy.observations\n",
    "WHERE {spatial_filter}\n",
    "\"\"\".format(spatial_filter=spatial_filter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189b8a5-d83c-4ee1-91dd-4cbc50782ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "filter_df.show()\n",
    "\n",
    "print(f\"Execution time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b0ad3-d741-4784-ba5d-070af6184bd6",
   "metadata": {},
   "source": [
    "Havasu supports all features of Apache Iceberg and also works with raster data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23dfd6-5846-436b-ae7d-6f33f9529e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
